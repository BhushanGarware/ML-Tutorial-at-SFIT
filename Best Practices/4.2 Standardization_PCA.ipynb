{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of standardization on PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the standardization affects PCA and a following supervised classification on the **Wine dataset**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.io.parsers.read_csv('./Datasets/wine.data', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset into a separate training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = df.values[:,1:]\n",
    "y = df.values[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Naive Bayes Classifier on Original Dataset Without applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# on non-standardized data\n",
    "model = GaussianNB()\n",
    "fit = model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred_y_train = model.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset without PCA ')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_y_train)))\n",
    "\n",
    "pred_y_test = model.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset without PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling - Standardization\n",
    "\n",
    "The result of **standardization** (or **Z-score normalization**) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with   \n",
    "\n",
    "$\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "where $\\mu$ is the mean (average) and $\\sigma$ is the standard deviation from the mean; standard scores (also called ***z*** scores) of the samples are calculated as follows:\n",
    "\n",
    "\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation} \n",
    "\n",
    "Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Naive Bayes Classifier on Std. Dataset Without applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on standardized data\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y_train_std = model.predict(X_train_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the Std training dataset without PCA ')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_y_train_std)))\n",
    "\n",
    "pred_y_test_std = model.predict(X_test_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the Std test dataset without PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_y_test_std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction via Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On non-standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On standardized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_std = PCA(n_components=2).fit(X_train_std)\n",
    "X_train_std = pca_std.transform(X_train_std)\n",
    "X_test_std = pca_std.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's qiuckly visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "\n",
    "for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax1.scatter(X_train[y_train==l, 0], X_train[y_train==l, 1],\n",
    "        color=c, \n",
    "        label='class %s' %l, \n",
    "        alpha=0.5,\n",
    "        marker=m\n",
    "        )\n",
    "\n",
    "for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax2.scatter(X_train_std[y_train==l, 0], X_train_std[y_train==l, 1],\n",
    "        color=c, \n",
    "        label='class %s' %l, \n",
    "        alpha=0.5,\n",
    "        marker=m\n",
    "        )\n",
    "\n",
    "ax1.set_title('Transformed NON-standardized training dataset after PCA')    \n",
    "ax2.set_title('Transformed standardized training dataset after PCA')    \n",
    "    \n",
    "for ax in (ax1, ax2):\n",
    "\n",
    "    ax.set_xlabel('1st principal component')\n",
    "    ax.set_ylabel('2nd principal component')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the classification accuracy with and without standardization with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on standardized data\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "pred_y_train = model.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy on Training dataset for PCA without std ')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_y_train)))\n",
    "\n",
    "pred_y_test = model.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy on Test dataset for PCA without std')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on standardized data\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_std, y_train)\n",
    "\n",
    "\n",
    "pred_y_train_std = model.predict(X_train_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the Std training dataset with PCA ')\n",
    "print('{:.2%}'.format(metrics.accuracy_score(y_train, pred_y_train_std)))\n",
    "\n",
    "pred_y_test_std = model.predict(X_test_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the Std test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_y_test_std)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
